#!/usr/bin/env python3
"""
Usage:
  word-clique1 [options] [<file>...]

Options:
  --unique FILE      Write anagram unique words to FILE
  -h --help          Show this screen.
  --version          Show version.
"""
import sys
from pathlib import Path
from operator import ior
from functools import reduce
from typing import Set, Dict, List
from time import monotonic

# External modules
from docopt import docopt	# type: ignore
from tqdm import tqdm

FILENAME = Path("~/Downloads/words.txt").expanduser()
WORD_LENGTH = 5
DEBUG = True

def analyze(filenames: List[Path], word_length, unifile: str = None) -> None:
    letter_set: Set[str] = set()
    cwords: Dict[str, List[str]] = {}

    load_time = monotonic()
    words_seen: Set[str] = set()
    for filename in filenames:
        with open(filename) as file:
            for word in tqdm(file):
                word = word.strip()
                if len(word) != word_length or not word.isalpha():
                    continue
                cset = set(word.lower())
                if len(cset) != word_length or word in words_seen:
                    continue
                words_seen.add(word)
                cword = "".join(sorted(cset))
                if cword not in cwords:
                    cwords[cword] = []
                    letter_set |= cset
                cwords[cword].append(word)

    letters = "".join(sorted(letter_set))
    print(f"{len(letters)} letters:", letters, file=sys.stderr)
    nr_words = len(letters) // word_length
    if nr_words <= 1:
        assert nr_words == 1
        raise NotImplementedError("{nr_words=} is not supported (and trivial)")
    nr_words - 1
    letter_bits = { letter: 1 << i for i, letter in enumerate(letters) }
    bwords = { reduce(ior, (letter_bits[l] for l in cword)): cword for cword in cwords }

    def to_word(bword: int) -> str:
        return cwords[bwords[bword]][0]

    nr_bwords = len(bwords)
    assert nr_bwords == len(cwords)
    print(f"{len(words_seen)} words, {nr_bwords} unique anagrams", file = sys.stderr)
    if unifile is not None:
        with open(unifile, "w") as uni_handle:
            for words in tqdm(cwords.values()):
                print(words[0], file = uni_handle)

    # Starting with relatively isolated nodes is faster
    # Starting with very connected nodes is a disaster
    bword_keys = { bword: sum(b & bword == 0 for b in bwords) for bword in bwords}
    # print(bword_keys)
    bword_list = sorted(bwords, key = lambda x: bword_keys[x])
    # print(list(map(to_word, bword_list)), sys=stderr)

    base_time = monotonic()
    print(f"Prepare after {base_time - load_time:4.1f}s", file = sys.stderr)

    start_time = monotonic()
    selected = [0] * nr_words

    def explore(i, candidates, used_letters):
        i1 = i+1
        left = nr_words - i1
        loop = tqdm(candidates) if i == 0 else candidates
        for j, bword in enumerate(loop):
            selected[i] = bword
            # print(f"{'  '*i}explore({i=}, {list(map(to_word, selected[:i+1]))})")
            if left == 0:
                words = list(map(to_word, selected))
                elapsed = monotonic() - start_time
                print(f"{elapsed:7.1f}: {words}")
                return

            used = used_letters | bword
            possible = [bword2 for bword2 in candidates[j+1:] if bword2 & used == 0]
            if len(possible) < left:
                continue
            explore(i1, possible, used)

    print(f"Start after {start_time - base_time:4.1f}s", file = sys.stderr)
    explore(0, bword_list, 0)

if __name__ == '__main__':
    args = docopt(__doc__, version=f"word-clique 1.000")
    analyze(args["<file>"] or [FILENAME], WORD_LENGTH, unifile = args["--unique"])
